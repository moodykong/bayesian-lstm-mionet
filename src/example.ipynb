{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================\n",
      "Default dataset parameters:\n",
      "ode_system = pendulum\n",
      "t_max = 20\n",
      "step_size = 0.01\n",
      "n_sample = 10\n",
      "x_init_pts = [[-3.14159265  3.14159265]\n",
      " [-8.          8.        ]]\n",
      "ctr_func = gaussian\n",
      "datafile_path = data/pendulum_ctr_gaussian_N_10_T20.npy\n",
      "\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from argparse import ArgumentParser\n",
    "import os, sys\n",
    "from importlib import reload\n",
    "from config import data_config\n",
    "from utils.args_parser import add_data_args\n",
    "\n",
    "# Reload arg_parser because Jupyter notebook does not reload it automatically\n",
    "reload(data_config)\n",
    "\n",
    "# Add arguments to parser\n",
    "parser = ArgumentParser()\n",
    "parser = add_data_args(parser)\n",
    "\n",
    "# Print default dataset parameters\n",
    "args = parser.parse_args([])\n",
    "print(\"\\n=====================\")\n",
    "print(\"Default dataset parameters:\")\n",
    "for arg in vars(args):\n",
    "    print(arg, '=', getattr(args, arg))\n",
    "print(\"\\n=====================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================\n",
      "Running database_generator.py with arguments:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 100 data ...:   4%|▍         | 4/100 [00:00<00:19,  5.02it/s]/PENDULUM/src/config/data_config.py:35: RuntimeWarning: invalid value encountered in subtract\n",
      "  u - b * omega - 0.5 * m * l * g * np.sin(theta)\n",
      "/PENDULUM/src/config/data_config.py:35: RuntimeWarning: invalid value encountered in sin\n",
      "  u - b * omega - 0.5 * m * l * g * np.sin(theta)\n",
      "Generating 100 data ...: 100%|██████████| 100/100 [00:21<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved in data/pendulum_ctr_grf_N_100_h001_T10_2.npy .\n"
     ]
    }
   ],
   "source": [
    "# Run database_generator.py with arguments\n",
    "print(\"\\n=====================\")\n",
    "print(\"Running database_generator.py with arguments:\")\n",
    "%run database_generator.py \\\n",
    "    --ode_system \"pendulum\" \\\n",
    "    --step_size 0.01 \\\n",
    "    --n_sample 100 \\\n",
    "    --ctr_func \"gaussian\" \\\n",
    "    --t_max 10 \\\n",
    "    --datafile_path \"data/pendulum_ctr_grf_N_100_h001_T10_2.npy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: database_generator.py [-h] [--ode_system {pendulum,lorentz}]\n",
      "                             [--t_max T_MAX] [--step_size STEP_SIZE]\n",
      "                             [--n_sample N_SAMPLE]\n",
      "                             [--x_init_pts [X_INIT_PTS ...]]\n",
      "                             [--ctr_func {designate,gaussian}]\n",
      "                             [--datafile_path DATAFILE_PATH]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --ode_system {pendulum,lorentz}\n",
      "                        ODE system to use.\n",
      "  --t_max T_MAX         Maximum time (seconds) of the trajectory.\n",
      "  --step_size STEP_SIZE\n",
      "                        Step size (seconds) of the trajectory.\n",
      "  --n_sample N_SAMPLE   Number of the trajectories to generate.\n",
      "  --x_init_pts [X_INIT_PTS ...]\n",
      "                        The range of initial points of the trajectories. Two\n",
      "                        floats define the range of each state component.\n",
      "  --ctr_func {designate,gaussian}\n",
      "                        Control function to use.\n",
      "  --datafile_path DATAFILE_PATH\n",
      "                        Path to save the data.\n"
     ]
    }
   ],
   "source": [
    "%run database_generator.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(5000, 999)\n",
      "(5000, 999)\n",
      "(5000, 999)\n",
      "(999, 1)\n",
      "(999, 1, 5000)\n",
      "(999, 2, 5000)\n",
      "(5000,)\n",
      "(4820,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datafile_path = \"/PENDULUM/archive/data/pendulum_u_random_init_a_001.pkl\"\n",
    "data = pd.read_pickle(datafile_path)\n",
    "print(type(data[\"u\"]))\n",
    "print(data[\"u\"].shape)\n",
    "print(data[\"theta\"].shape)\n",
    "print(data[\"omega\"].shape)\n",
    "print(data[\"t\"].shape)\n",
    "data_new = dict()\n",
    "data_new[\"u\"] = np.expand_dims(data[\"u\"].T, axis=1)\n",
    "data_new[\"x\"] = np.dstack((data[\"theta\"].T, data[\"omega\"].T)).transpose(0, 2, 1)\n",
    "data_new[\"t\"] = data[\"t\"]\n",
    "print(data_new[\"u\"].shape)\n",
    "print(data_new[\"x\"].shape)\n",
    "idxs_all = np.arange(data_new[\"u\"].shape[2])\n",
    "idxs_nan_u = np.isnan(data_new[\"u\"]).sum(axis=(0,1)) \n",
    "idxs_nan_x = np.isnan(data_new[\"x\"]).sum(axis=(0,1))\n",
    "idxs_nan = idxs_nan_u + idxs_nan_x\n",
    "idxs_valid = idxs_all[idxs_nan==0]\n",
    "print(idxs_all.shape)\n",
    "print(idxs_valid.shape)\n",
    "data_new[\"u\"] = data_new[\"u\"][:, :, idxs_valid]\n",
    "data_new[\"x\"] = data_new[\"x\"][:, :, idxs_valid]\n",
    "np.save(\"/PENDULUM/src/data/pendulum_ctr_grf_N_5000_T10_2.npy\", data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(120, 999)\n",
      "(120, 999)\n",
      "(120, 999)\n",
      "(999, 1)\n",
      "(999, 1, 120)\n",
      "(999, 2, 120)\n",
      "(120,)\n",
      "(115,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datafile_path = \"/PENDULUM/archive/data/pendulum_u_test_random_init_a_001_stat.pkl\"\n",
    "data = pd.read_pickle(datafile_path)\n",
    "print(type(data[\"u\"]))\n",
    "print(data[\"u\"].shape)\n",
    "print(data[\"theta\"].shape)\n",
    "print(data[\"omega\"].shape)\n",
    "print(data[\"t\"].shape)\n",
    "data_new = dict()\n",
    "data_new[\"u\"] = np.expand_dims(data[\"u\"].T, axis=1)\n",
    "data_new[\"x\"] = np.dstack((data[\"theta\"].T, data[\"omega\"].T)).transpose(0, 2, 1)\n",
    "data_new[\"t\"] = data[\"t\"]\n",
    "print(data_new[\"u\"].shape)\n",
    "print(data_new[\"x\"].shape)\n",
    "idxs_all = np.arange(data_new[\"u\"].shape[2])\n",
    "idxs_nan_u = np.isnan(data_new[\"u\"]).sum(axis=(0,1)) \n",
    "idxs_nan_x = np.isnan(data_new[\"x\"]).sum(axis=(0,1))\n",
    "idxs_nan = idxs_nan_u + idxs_nan_x\n",
    "idxs_valid = idxs_all[idxs_nan==0]\n",
    "print(idxs_all.shape)\n",
    "print(idxs_valid.shape)\n",
    "data_new[\"u\"] = data_new[\"u\"][:, :, idxs_valid]\n",
    "data_new[\"x\"] = data_new[\"x\"][:, :, idxs_valid]\n",
    "np.save(\"/PENDULUM/src/data/pendulum_ctr_grf_N_100_T10_2.npy\", data_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([2300, 999])\n",
      "Data memory: 8.82 MB\n",
      "tensor([-56.0552,   0.7273,  -0.7407,  -2.2929, -11.3510,   1.6141,  12.0482,\n",
      "         -0.8410,  -1.5140,   1.2153,  -0.6183,  23.9170,  -0.1437,   0.6723,\n",
      "        -41.4200,  -0.4753, -47.2333,  -2.3165,  -0.2414,   1.7439])\n",
      "Shapes for LSTM-MIONet training input=(2300, 999, 1), x_n=(2300, 1), x_next=(2300, 1)\n",
      "Data memory size: 8 MB\n",
      "tensor([-56.0552,   0.7273,  -0.7407,  -2.2929, -11.3510,   1.6141,  12.0482,\n",
      "         -0.8410,  -1.5140,   1.2153,  -0.6183,  23.9170,  -0.1437,   0.6723,\n",
      "        -41.4200,  -0.4753, -47.2333,  -2.3165,  -0.2414,   1.7439])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.nn_lib import Customize_Dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from utils.data_utils import split_dataset, prepare_local_predict_dataset, scale_and_to_tensor, Dataset_Stat, Dataset_Torch\n",
    "from config import train_config\n",
    "\n",
    "seed = 999\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "datafile_path = \"/PENDULUM/archive/data/pendulum_u_test_random_init_a_001_stat.pkl\"\n",
    "config = dict()\n",
    "config.update(train_config.get_config())\n",
    "\n",
    "dataset_o = Customize_Dataset(\n",
    "        filepath=datafile_path,\n",
    "        search_len=10,\n",
    "        search_num=20,\n",
    "        use_padding=True,\n",
    "        search_random = True,\n",
    "        device=torch.device(\"cpu\"),\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "    )\n",
    "\n",
    "print(dataset_o.metadata[0:20,-2])\n",
    "\n",
    "datafile_path = \"/PENDULUM/src/data/pendulum_ctr_grf_N_100_T10_2.npy\"\n",
    "dataset = np.load(datafile_path, allow_pickle=True).item()\n",
    "data, _ = split_dataset(\n",
    "        dataset, test_size=0., verbose=False\n",
    "    )\n",
    "input_masked, x_n, x_next, t_params = prepare_local_predict_dataset(\n",
    "        data=data,\n",
    "        state_component=config[\"state_component\"],\n",
    "        t_max=config[\"t_max\"],\n",
    "        search_len=config[\"search_len\"],\n",
    "        search_num=config[\"search_num\"],\n",
    "        search_random=config[\"search_random\"],\n",
    "        offset=config[\"offset\"],\n",
    "        verbose=config[\"verbose\"],\n",
    "    )\n",
    "data_stat = Dataset_Stat(input_masked, x_n, x_next, t_params)\n",
    "input_masked, x_n, x_next, t_params = scale_and_to_tensor(\n",
    "        data_stat, scale_mode=config[\"scale_mode\"]\n",
    "    )\n",
    "data_torch = Dataset_Torch(input_masked, x_n, x_next, t_params)\n",
    "print(data_torch.x_next[0:20,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================\n",
      "Running database_generator.py with arguments:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 100 data ...: 100%|██████████| 100/100 [00:26<00:00,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved in data/lorentz_N_100_h001_T20.npy .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run database_generator.py with arguments\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n=====================\")\n",
    "print(\"Running database_generator.py with arguments:\")\n",
    "%run database_generator.py \\\n",
    "    --ode_system \"lorentz\" \\\n",
    "    --step_size 0.01 \\\n",
    "    --n_sample 100 \\\n",
    "    --t_max 20 \\\n",
    "    --x_init_pts -17 20 -23 28 0 50 \\\n",
    "    --datafile_path \"data/lorentz_N_100_h001_T20.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(998, 2)\n",
      "(998, 2)\n",
      "3.30081828012151\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Read txt file\n",
    "datafile_path_pred = \"../archive/data/pendulum_infer_theta_single_chris_osc_pred.txt\"\n",
    "y_pred = pd.read_csv(datafile_path_pred, sep=\" \")\n",
    "datafile_path_true = \"../archive/data/pendulum_infer_theta_single_chris_osc_true.txt\"\n",
    "y_true = pd.read_csv(datafile_path_true, sep=\" \")\n",
    "print(y_pred.shape)\n",
    "print(y_true.shape)\n",
    "L2_error = np.linalg.norm(y_pred.iloc[:,1] - y_true.iloc[:,1], axis=0) / np.linalg.norm(y_true.iloc[:,1], axis=0)\n",
    "print(L2_error*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
