{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================\n",
      "Default dataset parameters:\n",
      "ode_system = pendulum\n",
      "t_max = 20\n",
      "step_size = 0.01\n",
      "n_sample = 10\n",
      "x_init_pts = [[-3.14159265  3.14159265]\n",
      " [-8.          8.        ]]\n",
      "ctr_func = gaussian\n",
      "datafile_path = data/pendulum_ctr_gaussian_N_10_T20.npy\n",
      "\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from argparse import ArgumentParser\n",
    "import os, sys\n",
    "from importlib import reload\n",
    "from config import data_config\n",
    "from utils.args_parser import add_data_args\n",
    "\n",
    "# Reload arg_parser because Jupyter notebook does not reload it automatically\n",
    "reload(data_config)\n",
    "\n",
    "# Add arguments to parser\n",
    "parser = ArgumentParser()\n",
    "parser = add_data_args(parser)\n",
    "\n",
    "# Print default dataset parameters\n",
    "args = parser.parse_args([])\n",
    "print(\"\\n=====================\")\n",
    "print(\"Default dataset parameters:\")\n",
    "for arg in vars(args):\n",
    "    print(arg, '=', getattr(args, arg))\n",
    "print(\"\\n=====================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================\n",
      "Running database_generator.py with arguments:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 1 data ...: 100%|██████████| 1/1 [00:00<00:00,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved in data/pendulum_ctr_grf_N_1_T10.npy .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run database_generator.py with arguments\n",
    "print(\"\\n=====================\")\n",
    "print(\"Running database_generator.py with arguments:\")\n",
    "%run database_generator.py --n_sample 1 --ctr_func \"gaussian\" --t_max 10 --datafile_path \"data/pendulum_ctr_grf_N_1_T10.npy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all available 1 GPUs.\n",
      "Shapes for LSTM-MIONet training input=(18000, 999, 1), x_n=(18000, 1), x_next=(18000, 1)\n",
      "Shapes for LSTM-MIONet training input=(2000, 999, 1), x_n=(2000, 1), x_next=(2000, 1)\n",
      "Data memory size: 68 MB\n",
      "Data memory size: 7 MB\n",
      "LSTM_MIONet_Static(\n",
      "  (layernorm): LayerNorm((150,), eps=1e-05, elementwise_affine=True)\n",
      "  (layernorm_x_n): LayerNorm((150,), eps=1e-05, elementwise_affine=True)\n",
      "  (layernorm_delta_t): LayerNorm((150,), eps=1e-05, elementwise_affine=True)\n",
      "  (lstm): LSTM(150, 10, num_layers=2, batch_first=True)\n",
      "  (cell_mlp): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=150, bias=True)\n",
      "  )\n",
      "  (input_mlp): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=150, bias=True)\n",
      "  )\n",
      "  (delta_t_mlp): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=150, bias=True)\n",
      "  )\n",
      "  (x_n_mlp): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=150, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "***** Training with Adam Optimizer for 1000 epochs and using 18000 data samples*****\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "  3%|▎         | 31/1000 [04:47<2:24:44,  8.96s/it, Train=0.677, Val=0.163, Best_train=0.186, Best_Val=0.0899]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00031: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 62/1000 [09:28<2:13:42,  8.55s/it, Train=0.0946, Val=0.0809, Best_train=0.0663, Best_Val=0.052] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00062: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 76/1000 [11:33<2:11:47,  8.56s/it, Train=0.096, Val=0.0665, Best_train=0.0585, Best_Val=0.0507] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00076: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 93/1000 [14:05<2:07:47,  8.45s/it, Train=0.0656, Val=0.0734, Best_train=0.0497, Best_Val=0.0474]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00093: reducing learning rate of group 0 to 6.2500e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 113/1000 [17:01<2:06:56,  8.59s/it, Train=0.0503, Val=0.0616, Best_train=0.0485, Best_Val=0.047] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00113: reducing learning rate of group 0 to 3.1250e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 127/1000 [19:02<2:04:14,  8.54s/it, Train=0.048, Val=0.049, Best_train=0.0465, Best_Val=0.0464]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00127: reducing learning rate of group 0 to 1.5625e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 150/1000 [22:34<2:07:55,  9.03s/it, Train=0.046, Val=0.0474, Best_train=0.0454, Best_Val=0.0456] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/PENDULUM/src/train.py:159\u001b[0m\n\u001b[1;32m    155\u001b[0m     run(config)\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 159\u001b[0m     main()\n",
      "File \u001b[0;32m/PENDULUM/src/train.py:155\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m os\u001b[39m.\u001b[39mmakedirs(config[\u001b[39m\"\u001b[39m\u001b[39mcheckpoint_path\u001b[39m\u001b[39m\"\u001b[39m], exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    154\u001b[0m \u001b[39m# Run the program\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m run(config)\n",
      "File \u001b[0;32m/PENDULUM/src/train.py:128\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[39mprint\u001b[39m(model)\n\u001b[1;32m    124\u001b[0m \u001b[39m###################################\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39m# Step 8: train the model\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39m###################################\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m execute_train(\n\u001b[1;32m    129\u001b[0m     config\u001b[39m=\u001b[39;49mconfig, model\u001b[39m=\u001b[39;49mmodel, dataset\u001b[39m=\u001b[39;49mtrain_data_torch, device\u001b[39m=\u001b[39;49mtorch_utils\u001b[39m.\u001b[39;49mdevice\n\u001b[1;32m    130\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[39m###################################\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m# Step 9: test the model\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39m###################################\u001b[39;00m\n\u001b[1;32m    136\u001b[0m execute_test(config\u001b[39m=\u001b[39mconfig, model\u001b[39m=\u001b[39mmodel, dataset\u001b[39m=\u001b[39mtest_data_torch)\n",
      "File \u001b[0;32m/PENDULUM/src/optim/supervisor.py:113\u001b[0m, in \u001b[0;36mexecute_train\u001b[0;34m(config, model, dataset, device)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39m# step c: compute gradients and backpropagate\u001b[39;00m\n\u001b[1;32m    112\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 113\u001b[0m scalar\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    115\u001b[0m \u001b[39m# step d: optimize\u001b[39;00m\n\u001b[1;32m    116\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run train.py --datafile_path \"data/pendulum_ctr_grf_N_5000_T10.npy\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(5000, 999)\n",
      "(5000, 999)\n",
      "(5000, 999)\n",
      "(999, 1)\n",
      "(999, 1, 5000)\n",
      "(999, 2, 5000)\n",
      "(5000,)\n",
      "(4820,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datafile_path = \"/PENDULUM/archive/data/pendulum_u_random_init_a_001.pkl\"\n",
    "data = pd.read_pickle(datafile_path)\n",
    "print(type(data[\"u\"]))\n",
    "print(data[\"u\"].shape)\n",
    "print(data[\"theta\"].shape)\n",
    "print(data[\"omega\"].shape)\n",
    "print(data[\"t\"].shape)\n",
    "data_new = dict()\n",
    "data_new[\"u\"] = np.expand_dims(data[\"u\"].T, axis=1)\n",
    "data_new[\"x\"] = np.dstack((data[\"theta\"].T, data[\"omega\"].T)).transpose(0, 2, 1)\n",
    "data_new[\"t\"] = data[\"t\"]\n",
    "print(data_new[\"u\"].shape)\n",
    "print(data_new[\"x\"].shape)\n",
    "idxs_all = np.arange(data_new[\"u\"].shape[2])\n",
    "idxs_nan_u = np.isnan(data_new[\"u\"]).sum(axis=(0,1)) \n",
    "idxs_nan_x = np.isnan(data_new[\"x\"]).sum(axis=(0,1))\n",
    "idxs_nan = idxs_nan_u + idxs_nan_x\n",
    "idxs_valid = idxs_all[idxs_nan==0]\n",
    "print(idxs_all.shape)\n",
    "print(idxs_valid.shape)\n",
    "data_new[\"u\"] = data_new[\"u\"][:, :, idxs_valid]\n",
    "data_new[\"x\"] = data_new[\"x\"][:, :, idxs_valid]\n",
    "np.save(\"/PENDULUM/src/data/pendulum_ctr_grf_N_5000_T10_2.npy\", data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(120, 999)\n",
      "(120, 999)\n",
      "(120, 999)\n",
      "(999, 1)\n",
      "(999, 1, 120)\n",
      "(999, 2, 120)\n",
      "(120,)\n",
      "(115,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datafile_path = \"/PENDULUM/archive/data/pendulum_u_test_random_init_a_001_stat.pkl\"\n",
    "data = pd.read_pickle(datafile_path)\n",
    "print(type(data[\"u\"]))\n",
    "print(data[\"u\"].shape)\n",
    "print(data[\"theta\"].shape)\n",
    "print(data[\"omega\"].shape)\n",
    "print(data[\"t\"].shape)\n",
    "data_new = dict()\n",
    "data_new[\"u\"] = np.expand_dims(data[\"u\"].T, axis=1)\n",
    "data_new[\"x\"] = np.dstack((data[\"theta\"].T, data[\"omega\"].T)).transpose(0, 2, 1)\n",
    "data_new[\"t\"] = data[\"t\"]\n",
    "print(data_new[\"u\"].shape)\n",
    "print(data_new[\"x\"].shape)\n",
    "idxs_all = np.arange(data_new[\"u\"].shape[2])\n",
    "idxs_nan_u = np.isnan(data_new[\"u\"]).sum(axis=(0,1)) \n",
    "idxs_nan_x = np.isnan(data_new[\"x\"]).sum(axis=(0,1))\n",
    "idxs_nan = idxs_nan_u + idxs_nan_x\n",
    "idxs_valid = idxs_all[idxs_nan==0]\n",
    "print(idxs_all.shape)\n",
    "print(idxs_valid.shape)\n",
    "data_new[\"u\"] = data_new[\"u\"][:, :, idxs_valid]\n",
    "data_new[\"x\"] = data_new[\"x\"][:, :, idxs_valid]\n",
    "np.save(\"/PENDULUM/src/data/pendulum_ctr_grf_N_100_T10_2.npy\", data_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([2300, 999])\n",
      "Data memory: 8.82 MB\n",
      "tensor([-56.0552,   0.7273,  -0.7407,  -2.2929, -11.3510,   1.6141,  12.0482,\n",
      "         -0.8410,  -1.5140,   1.2153,  -0.6183,  23.9170,  -0.1437,   0.6723,\n",
      "        -41.4200,  -0.4753, -47.2333,  -2.3165,  -0.2414,   1.7439])\n",
      "Shapes for LSTM-MIONet training input=(2300, 999, 1), x_n=(2300, 1), x_next=(2300, 1)\n",
      "Data memory size: 8 MB\n",
      "tensor([-56.0552,   0.7273,  -0.7407,  -2.2929, -11.3510,   1.6141,  12.0482,\n",
      "         -0.8410,  -1.5140,   1.2153,  -0.6183,  23.9170,  -0.1437,   0.6723,\n",
      "        -41.4200,  -0.4753, -47.2333,  -2.3165,  -0.2414,   1.7439])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.nn_lib import Customize_Dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from utils.data_utils import split_dataset, prepare_local_predict_dataset, scale_and_to_tensor, Dataset_Stat, Dataset_Torch\n",
    "from config import train_config\n",
    "\n",
    "seed = 999\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "datafile_path = \"/PENDULUM/archive/data/pendulum_u_test_random_init_a_001_stat.pkl\"\n",
    "config = dict()\n",
    "config.update(train_config.get_config())\n",
    "\n",
    "dataset_o = Customize_Dataset(\n",
    "        filepath=datafile_path,\n",
    "        search_len=10,\n",
    "        search_num=20,\n",
    "        use_padding=True,\n",
    "        search_random = True,\n",
    "        device=torch.device(\"cpu\"),\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "    )\n",
    "\n",
    "print(dataset_o.metadata[0:20,-2])\n",
    "\n",
    "datafile_path = \"/PENDULUM/src/data/pendulum_ctr_grf_N_100_T10_2.npy\"\n",
    "dataset = np.load(datafile_path, allow_pickle=True).item()\n",
    "data, _ = split_dataset(\n",
    "        dataset, test_size=0., verbose=False\n",
    "    )\n",
    "input_masked, x_n, x_next, t_params = prepare_local_predict_dataset(\n",
    "        data=data,\n",
    "        state_component=config[\"state_component\"],\n",
    "        t_max=config[\"t_max\"],\n",
    "        search_len=config[\"search_len\"],\n",
    "        search_num=config[\"search_num\"],\n",
    "        search_random=config[\"search_random\"],\n",
    "        offset=config[\"offset\"],\n",
    "        verbose=config[\"verbose\"],\n",
    "    )\n",
    "data_stat = Dataset_Stat(input_masked, x_n, x_next, t_params)\n",
    "input_masked, x_n, x_next, t_params = scale_and_to_tensor(\n",
    "        data_stat, scale_mode=config[\"scale_mode\"]\n",
    "    )\n",
    "data_torch = Dataset_Torch(input_masked, x_n, x_next, t_params)\n",
    "print(data_torch.x_next[0:20,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
